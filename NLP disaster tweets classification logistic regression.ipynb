{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP72EyKRxz8X0QWNnnAUSC3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnySunkel/BlueSky/blob/master/NLP%20disaster%20tweets%20classification%20logistic%20regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4NwmNoIaolq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "92e217a1-26b2-4309-f867-03e902b8c460"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "df = pd.read_csv('nlp_disaster_tweets.csv')\n",
        "\n",
        "X_train = df.loc[:6090, 'text'].values\n",
        "y_train = df.loc[:6090, 'target'].values\n",
        "X_test = df.loc[6090:, 'text'].values\n",
        "y_test = df.loc[6090:, 'target'].values\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "  return text.split()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "  return[porter.stem(word) for word in text.split()]\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents = None,\n",
        "                        lowercase = False,\n",
        "                        preprocessor = None)\n",
        "\n",
        "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer,\n",
        "                                  tokenizer_porter],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              {'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer,\n",
        "                                   tokenizer_porter],\n",
        "               'vect__use_idf': [False],\n",
        "               'vect__norm': [None],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]}\n",
        "              ]\n",
        "\n",
        "lr_tfidf = Pipeline([('vect', tfidf),\n",
        "                     ('clf',\n",
        "                      LogisticRegression(random_state = 0))])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf,\n",
        "                           param_grid,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 5,\n",
        "                           verbose = 1,\n",
        "                           n_jobs = -1)\n",
        "\n",
        "gs_lr_tfidf.fit(X_train, y_train)\n",
        "\n",
        "print('Best parameter set: %s' % gs_lr_tfidf.best_params_)\n",
        "\n",
        "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
        "\n",
        "clf = gs_lr_tfidf.best_estimator_\n",
        "\n",
        "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:   36.3s\n",
            "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  3.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameter set: {'clf__C': 1.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x7f0bfcf09c80>}\n",
            "CV Accuracy: 0.711\n",
            "Test Accuracy: 0.804\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}